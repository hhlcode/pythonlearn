{"classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Natural Language :: Chinese (Simplified)", "Natural Language :: Chinese (Traditional)", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.7", "Topic :: Text Processing", "Topic :: Text Processing :: Indexing", "Topic :: Text Processing :: Linguistic"], "extensions": {"python.details": {"contacts": [{"email": "shaohao97@gmail.com", "name": "Sun, Junyi, deepcs233", "role": "author"}], "document_names": {"description": "DESCRIPTION.rst"}, "project_urls": {"Home": "https://github.com/deepcs233/jieba_fast"}}}, "generator": "bdist_wheel (0.29.0)", "keywords": ["NLP", "tokenizing", "Chinese", "word", "segementation"], "license": "MIT", "metadata_version": "2.0", "name": "jieba-fast", "summary": "Use C and Swig to Speed up jieba<Chinese Words Segementation Utilities>", "version": "0.49"}