{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import thulac\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('人工分词的药食同源.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vob ():\n",
    "    \"\"\"\n",
    "    Python 消除换行符一定.replace('\\n','').replace('\\r','') \n",
    "    将词表转化为一个list\n",
    "    \"\"\"\n",
    "    with open ('UserDict.txt','r') as f:\n",
    "        vocabL = f.readlines() #读取的结果是list \n",
    "        vocabSet = []\n",
    "        for i in vocabL:#删除换行符\n",
    "            try:\n",
    "                i = i.replace('\\n','').replace('\\r','')\n",
    "                vocabSet.append(i)\n",
    "            except:\n",
    "                continue\n",
    "        f.close()\n",
    "        return  vocabSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSet = vob()\n",
    "abstract = df['摘要']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cut (abstract,vocabSet):\n",
    "    times = 0 \n",
    "    for i in range(10):\n",
    "        time_start = time.time()\n",
    "        thul = thulac.thulac(seg_only=True)\n",
    "#         thul = thulac.thulac(user_dict ='UserDict.txt', seg_only=True)  #只进行分词，不进行词性标注\n",
    "        c = []\n",
    "        for ab in abstract:#把每个文章的摘要单独提取出来\n",
    "            strs = [] #把每个文章的用到的药材提取出来放在列表中\n",
    "            # print(ab)\n",
    "            contents = thul.cut(str(ab),text=True)#默认模式进行分词\n",
    "            # print(contents)判断是否分词了\n",
    "            # exit()\n",
    "            for content in contents.split():\n",
    "                # print(content)\n",
    "            # exit()\n",
    "                if content  in vocabSet and content != '药食同源' and content not in strs:  #判断是否在词表里面，提取中药材的关键词\n",
    "                    # print(content)\n",
    "                    strs.append(content)\n",
    "                    # print(strs)\n",
    "                    # exit()\n",
    "            strs = str(strs).replace(\"'\",'').replace('[','').replace(']','')#将其转化为一个一维的列表目的就是为转化为DataFrame类型\n",
    "            c.append(strs)#将每一行分词结果添加进list\n",
    "            strs = []\n",
    "        time_end = time.time()\n",
    "        time_sum = time_end-time_start\n",
    "        times += time_sum\n",
    "    print(times/10)\n",
    "    # 没引进词表的分词时间18.879502367973327\n",
    "    # 引进词表的分词时间  19.681711983680724\n",
    "    return  c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "Model loaded succeed\n",
      "18.879502367973327\n"
     ]
    }
   ],
   "source": [
    "text = cut(abstract,vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
